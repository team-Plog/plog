from fastapi import APIRouter, Path, Query
from fastapi.responses import StreamingResponse

from app.models import get_db
from app.models.influxdb.database import client
import asyncio
import json
import logging
import pytz
from typing import List, Dict, Any, Optional
from datetime import datetime
from pydantic import BaseModel, Field

from app.models.sqlite.models import TestHistoryModel, StageHistoryModel, ScenarioHistoryModel
from app.services.infrastructure.server_infra_service import get_job_pods_with_service_types
from app.services.testing.test_history_service import get_test_history_by_job_name
from app.sse.pod_spec_cache import get_pod_spec_cache
from app.sse.metrics_buffer import SmartMetricsBuffer
from app.models.sqlite.database import SessionLocal

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()
kst = pytz.timezone('Asia/Seoul')


# ========== Pydantic ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ëª¨ë¸ ==========

class K6OverallMetrics(BaseModel):
    """k6 ì „ì²´ ë©”íŠ¸ë¦­"""
    tps: float = Field(..., description="Transactions Per Second (ì´ˆë‹¹ íŠ¸ëœì­ì…˜ ìˆ˜)")
    vus: int = Field(..., description="Virtual Users (ê°€ìƒ ì‚¬ìš©ì ìˆ˜)")
    response_time: float = Field(..., description="í‰ê·  ì‘ë‹µì‹œê°„ (ms)")
    error_rate: float = Field(..., description="ì˜¤ë¥˜ìœ¨ (%)")


class K6ScenarioMetrics(BaseModel):
    """k6 ì‹œë‚˜ë¦¬ì˜¤ë³„ ë©”íŠ¸ë¦­"""
    name: str = Field(..., description="ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„")
    scenario_tag: str = Field(..., description="ì‹œë‚˜ë¦¬ì˜¤ íƒœê·¸")
    tps: float = Field(..., description="Transactions Per Second")
    vus: int = Field(..., description="Virtual Users")
    response_time: float = Field(..., description="í‰ê·  ì‘ë‹µì‹œê°„ (ms)")
    error_rate: float = Field(..., description="ì˜¤ë¥˜ìœ¨ (%)")


class ResourceUsage(BaseModel):
    """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ì •ë³´"""
    cpu_percent: float = Field(..., description="CPU ì‚¬ìš©ë¥  (limit ê¸°ì¤€ %)")
    memory_percent: float = Field(..., description="Memory ì‚¬ìš©ë¥  (limit ê¸°ì¤€ %)")
    cpu_is_predicted: bool = Field(..., description="CPU ì‚¬ìš©ë¥ ì´ ì˜ˆì¸¡ê°’ì¸ì§€ ì—¬ë¶€")
    memory_is_predicted: bool = Field(..., description="Memory ì‚¬ìš©ë¥ ì´ ì˜ˆì¸¡ê°’ì¸ì§€ ì—¬ë¶€")


class ActualUsage(BaseModel):
    """ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰"""
    cpu_millicores: Optional[float] = Field(None, description="ì‹¤ì œ CPU ì‚¬ìš©ëŸ‰ (millicores)")
    memory_mb: Optional[float] = Field(None, description="ì‹¤ì œ Memory ì‚¬ìš©ëŸ‰ (MB)")


class ResourceSpecs(BaseModel):
    """Pod ë¦¬ì†ŒìŠ¤ ìŠ¤í™"""
    cpu_request_millicores: Optional[float] = Field(None, description="CPU ìš”ì²­ëŸ‰ (millicores)")
    cpu_limit_millicores: Optional[float] = Field(None, description="CPU ì œí•œëŸ‰ (millicores)")
    memory_request_mb: Optional[float] = Field(None, description="Memory ìš”ì²­ëŸ‰ (MB)")
    memory_limit_mb: Optional[float] = Field(None, description="Memory ì œí•œëŸ‰ (MB)")


class PredictionInfo(BaseModel):
    """ì˜ˆì¸¡ ëª¨ë¸ ì •ë³´"""
    cpu_streak: int = Field(..., description="CPU ì˜ˆì¸¡ ì—°ì† íšŸìˆ˜")
    memory_streak: int = Field(..., description="Memory ì˜ˆì¸¡ ì—°ì† íšŸìˆ˜")
    cpu_confidence: float = Field(..., description="CPU ì˜ˆì¸¡ ì‹ ë¢°ë„ (0.0-1.0)")
    memory_confidence: float = Field(..., description="Memory ì˜ˆì¸¡ ì‹ ë¢°ë„ (0.0-1.0)")


class ResourceMetrics(BaseModel):
    """ê°œë³„ Pod ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­"""
    pod_name: str = Field(..., description="Pod ì´ë¦„")
    service_type: str = Field(..., description="ì„œë¹„ìŠ¤ ìœ í˜• (SERVER, DATABASE)")
    usage: ResourceUsage = Field(..., description="ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ì •ë³´")
    actual_usage: ActualUsage = Field(..., description="ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰")
    specs: ResourceSpecs = Field(..., description="Pod ë¦¬ì†ŒìŠ¤ ìŠ¤í™")
    prediction_info: PredictionInfo = Field(..., description="ì˜ˆì¸¡ ëª¨ë¸ ì •ë³´")


class SSEMetricsResponse(BaseModel):
    """SSE ë©”íŠ¸ë¦­ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    timestamp: str = Field(..., description="ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œê°„ (ISO 8601 í˜•ì‹)")
    overall: K6OverallMetrics = Field(..., description="k6 ì „ì²´ ë©”íŠ¸ë¦­")
    scenarios: List[K6ScenarioMetrics] = Field(..., description="k6 ì‹œë‚˜ë¦¬ì˜¤ë³„ ë©”íŠ¸ë¦­")
    resources: Optional[List[ResourceMetrics]] = Field(None, description="ì„œë²„ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ (include=allì¼ ë•Œë§Œ)")
    error: Optional[str] = Field(None, description="ì˜¤ë¥˜ ë©”ì‹œì§€ (ì˜¤ë¥˜ ë°œìƒì‹œë§Œ)")


def get_scenario_names(job_name: str) -> List[str]:
    """job_nameìœ¼ë¡œ í™œì„± ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ë“¤ì„ ì¡°íšŒ"""
    try:
        query = f'''
            SHOW TAG VALUES FROM "http_reqs" 
            WITH KEY = "scenario" 
            WHERE "job_name" = '{job_name}'
        '''
        result = client.query(query)
        scenarios = [point['value'] for point in result.get_points() if 'value' in point]
        logger.info(f"Total scenarios found: {len(scenarios)} -> {scenarios}")
        return scenarios
    except Exception as e:
        logger.error(f"Error in get_scenario_names: {e}")
        return []


def get_overall_tps(job_name: str) -> float:
    """ì „ì²´ TPS ì¡°íšŒ"""
    try:
        query = f'''
            SELECT COUNT("value") as total_requests
            FROM "http_reqs"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('total_requests'):
            total_requests = points[0]['total_requests']
            tps = total_requests / 10.0
            return round(tps, 1)
        return 0.0
    except Exception as e:
        logger.error(f"Error in get_overall_tps: {e}")
        return 0.0


def get_overall_vus(job_name: str) -> int:
    """ì „ì²´ í™œì„± Virtual Users ì¡°íšŒ"""
    try:
        query = f'''
            SELECT LAST("value") as vus
            FROM "vus"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('vus') is not None:
            return int(points[0]['vus'])
        return 0
    except Exception as e:
        logger.error(f"Error in get_overall_vus: {e}")
        return 0


def get_overall_latency(job_name: str) -> float:
    """ì „ì²´ í‰ê·  ì‘ë‹µì‹œê°„ ì¡°íšŒ"""
    try:
        query = f'''
            SELECT MEAN("value") as latency
            FROM "http_req_duration"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('latency') is not None:
            return round(points[0]['latency'], 2)
        return 0.0
    except Exception as e:
        logger.error(f"Error in get_overall_latency: {e}")
        return 0.0


def get_overall_error_rate(job_name: str) -> float:
    """ì „ì²´ ì˜¤ë¥˜ìœ¨ ì¡°íšŒ"""
    try:
        total_query = f'''
            SELECT SUM("value") as total
            FROM "http_reqs"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
        '''
        error_query = f'''
            SELECT SUM("value") as errors
            FROM "http_req_failed"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
        '''
        
        total_result = client.query(total_query)
        error_result = client.query(error_query)
        
        total_points = list(total_result.get_points())
        error_points = list(error_result.get_points())
        
        total_count = total_points[0]['total'] if total_points else 0
        error_count = error_points[0]['errors'] if error_points else 0
        
        if total_count == 0:
            return 0.0
            
        return round((error_count / total_count) * 100, 2)
    except Exception as e:
        logger.error(f"Error in get_overall_error_rate: {e}")
        return 0.0


def get_scenario_tps(job_name: str, scenario_name: str) -> float:
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ TPS ì¡°íšŒ"""
    try:
        query = f'''
            SELECT COUNT("value") as total_requests
            FROM "http_reqs"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
              AND "scenario" = '{scenario_name}'
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('total_requests'):
            total_requests = points[0]['total_requests']
            tps = total_requests / 10.0
            return round(tps, 1)
        return 0.0
    except Exception as e:
        logger.error(f"Error in get_scenario_tps for '{scenario_name}': {e}")
        return 0.0


def get_scenario_vus(job_name: str, scenario_name: str) -> int:
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ í™œì„± Virtual Users ì¡°íšŒ"""
    try:
        query = f'''
            SELECT LAST("value") as vus
            FROM "vus"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
              AND "scenario" = '{scenario_name}'
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('vus') is not None:
            return int(points[0]['vus'])
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ VUS ì¡°íšŒê°€ ì‹¤íŒ¨í•˜ë©´ ì „ì²´ VUSë¥¼ ëŒ€ì‹  ë°˜í™˜
        return get_overall_vus(job_name)
    except Exception as e:
        logger.error(f"Error in get_scenario_vus for '{scenario_name}': {e}")
        return 0


def get_scenario_latency(job_name: str, scenario_name: str) -> float:
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ì‘ë‹µì‹œê°„ ì¡°íšŒ"""
    try:
        query = f'''
            SELECT MEAN("value") as latency
            FROM "http_req_duration"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
              AND "scenario" = '{scenario_name}'
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('latency') is not None:
            return round(points[0]['latency'], 2)
        return 0.0
    except Exception as e:
        logger.error(f"Error in get_scenario_latency for '{scenario_name}': {e}")
        return 0.0


def get_scenario_error_rate(job_name: str, scenario_name: str) -> float:
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì˜¤ë¥˜ìœ¨ ì¡°íšŒ"""
    try:
        total_query = f'''
            SELECT SUM("value") as total
            FROM "http_reqs"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
              AND "scenario" = '{scenario_name}'
        '''
        error_query = f'''
            SELECT SUM("value") as errors
            FROM "http_req_failed"
            WHERE time > now() - 10s
              AND "job_name" = '{job_name}'
              AND "scenario" = '{scenario_name}'
        '''
        
        total_result = client.query(total_query)
        error_result = client.query(error_query)
        
        total_points = list(total_result.get_points())
        error_points = list(error_result.get_points())

        total_count = total_points[0]['total'] if total_points else 0
        error_count = error_points[0]['errors'] if error_points else 0

        if total_count == 0:
            return 0.0
            
        return round((error_count / total_count) * 100, 2)
    except Exception as e:
        logger.error(f"Error in get_scenario_error_rate for '{scenario_name}': {e}")
        return 0.0


def collect_metrics_data(db, job_name: str, include_resources: bool = True) -> Dict[str, Any]:
    """ëª¨ë“  ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  í¬ë§·íŒ… (k6 + resource ë©”íŠ¸ë¦­)"""
    logger.info(f"Starting metrics collection for job: {job_name} (include_resources={include_resources})")

    # 1. ê¸°ì¡´ k6 ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    scenario_tags = get_scenario_names(job_name)

    # DB ì„¸ì…˜ì„ ì•ˆì „í•˜ê²Œ ê´€ë¦¬
    test_history = get_test_history_by_job_name(db, job_name)
    scenarios = test_history.scenarios

    logger.info(f"scenarios length: {len(scenarios)}, scenario tags length: {len(scenario_tags)}")

    duration_seconds = get_duration_seconds(test_history.tested_at)
    total_duration_seconds = get_total_duration_seconds(test_history)

    test_progress = {
        "duration_seconds": duration_seconds,
        "total_duration_seconds": total_duration_seconds,
        "progress_percentage": round(duration_seconds / total_duration_seconds * 100, 2) if duration_seconds <= total_duration_seconds else 100,
    }

    if get_overall_vus(job_name) == 0:
        result = {
            "timestamp": datetime.now(kst).isoformat(),
            "test_progress": test_progress,
            "overall": None,
            "scenarios": None,
            "is_complete": True
        }

        return result

    scenario_tag_name_map = {
        scenario.scenario_tag: scenario.name
        for scenario in scenarios
    }

    overall_metrics = {
        "tps": get_overall_tps(job_name),
        "vus": get_overall_vus(job_name), 
        "response_time": get_overall_latency(job_name),
        "error_rate": get_overall_error_rate(job_name)
    }
    
    scenario_list = []
    for scenario_tag in scenario_tags:
        scenario_list.append({
            "name": scenario_tag_name_map.get(scenario_tag),
            "scenario_tag": scenario_tag,
            "tps": get_scenario_tps(job_name, scenario_tag),
            "vus": get_scenario_vus(job_name, scenario_tag),
            "response_time": get_scenario_latency(job_name, scenario_tag),
            "error_rate": get_scenario_error_rate(job_name, scenario_tag)
        })
    
    # 2. ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡°
    result = {
        "timestamp": datetime.now(kst).isoformat(),
        "test_progress": test_progress,
        "overall": overall_metrics,
        "scenarios": scenario_list
    }
    
    # 3. Resource ë©”íŠ¸ë¦­ ì¶”ê°€ (ì˜µì…˜)
    if include_resources:
        try:
            resource_metrics = collect_resource_metrics(job_name)
            if resource_metrics:
                result["resources"] = resource_metrics  # ì§ì ‘ ë°°ì—´ í• ë‹¹
                logger.debug(f"Added {len(resource_metrics)} resource metrics for job {job_name}")
            else:
                # ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°°ì—´
                result["resources"] = []
                logger.debug(f"No resource metrics available for job {job_name}, using empty array")
        except Exception as e:
            logger.error(f"Error collecting resource metrics for job {job_name}: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë°°ì—´
            result["resources"] = []

    db.commit()
    return result


async def event_stream(job_name: str, include_resources: bool = True):
    """k6 ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° (resource ë©”íŠ¸ë¦­ í¬í•¨)"""
    logger.info(f"Starting SSE stream for job: {job_name} (include_resources={include_resources})")
    db = SessionLocal()

    while True:
        try:
            metrics_data = collect_metrics_data(db, job_name, include_resources)

            yield f"data: {json.dumps(metrics_data, ensure_ascii=False)}\n\n"

            # í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œ ì—°ê²° ì¢…ë£Œ
            if metrics_data.get("is_complete", False) and metrics_data.get("test_progress", {}).get("progress_percentage") == 100:
                logger.info(f"Test {job_name} completed (VUS=0), closing SSE connection")
                break

        except Exception as e:
            logger.error(f"Error in event_stream: {e}")
            error_data = {
                "timestamp": datetime.now(kst).isoformat(),
                "overall": {"tps": 0, "vus": 0, "response_time": 0, "error_rate": 0},
                "scenarios": [],
                "error": str(e)
            }
            
            # ì—ëŸ¬ ì‹œì—ë„ resources êµ¬ì¡° í¬í•¨ (ìš”ì²­ëœ ê²½ìš°)
            if include_resources:
                error_data["resources"] = []  # ì—ëŸ¬ ì‹œ ë¹ˆ ë°°ì—´
                
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        await asyncio.sleep(5)


@router.get('/sse/k6data/{job_name}', 
           summary="ğŸ”„ SSE ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìŠ¤íŠ¸ë¦¬ë°",
           description="""k6 ë¶€í•˜í…ŒìŠ¤íŠ¸ì™€ ì„œë²„ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì„ Server-Sent Eventsë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

**ì‘ë‹µ JSON ì˜ˆì‹œ:**
```json
{
  "timestamp": "2025-09-08T12:34:56.789+09:00",  // ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œê°„
  "overall": {
    "tps": 125.6,           // ì „ì²´ ì´ˆë‹¹ íŠ¸ëœì­ì…˜ ìˆ˜
    "vus": 50,              // ì „ì²´ ê°€ìƒ ì‚¬ìš©ì ìˆ˜
    "response_time": 145.2, // ì „ì²´ í‰ê·  ì‘ë‹µì‹œê°„(ms)
    "error_rate": 0.5       // ì „ì²´ ì˜¤ë¥˜ìœ¨(%)
  },
  "scenarios": [
    {
      "name": "get_users",     // ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„
      "tps": 62.3,            // ì‹œë‚˜ë¦¬ì˜¤ë³„ TPS
      "vus": 25,              // ì‹œë‚˜ë¦¬ì˜¤ë³„ VUS
      "response_time": 140.1, // ì‹œë‚˜ë¦¬ì˜¤ë³„ ì‘ë‹µì‹œê°„(ms)
      "error_rate": 0.2       // ì‹œë‚˜ë¦¬ì˜¤ë³„ ì˜¤ë¥˜ìœ¨(%)
    }
  ],
  "resources": [
    {
      "pod_name": "api-server-123",    // Pod ì´ë¦„
      "service_type": "SERVER",        // ì„œë¹„ìŠ¤ ìœ í˜•
      "usage": {
        "cpu_percent": 45.2,           // CPU ì‚¬ìš©ë¥ (limit ê¸°ì¤€ %)
        "memory_percent": 67.8,        // Memory ì‚¬ìš©ë¥ (limit ê¸°ì¤€ %)
        "cpu_is_predicted": false,     // CPU ì˜ˆì¸¡ê°’ ì—¬ë¶€
        "memory_is_predicted": false   // Memory ì˜ˆì¸¡ê°’ ì—¬ë¶€
      },
      "actual_usage": {
        "cpu_millicores": 452.5,       // ì‹¤ì œ CPU ì‚¬ìš©ëŸ‰(millicores)
        "memory_mb": 678.3             // ì‹¤ì œ Memory ì‚¬ìš©ëŸ‰(MB)
      },
      "specs": {
        "cpu_request_millicores": 500, // CPU ìš”ì²­ëŸ‰(millicores)
        "cpu_limit_millicores": 1000,  // CPU ì œí•œëŸ‰(millicores)
        "memory_request_mb": 512,      // Memory ìš”ì²­ëŸ‰(MB)
        "memory_limit_mb": 1024        // Memory ì œí•œëŸ‰(MB)
      }
    }
  ]
}
```

- **ì—…ë°ì´íŠ¸ ì£¼ê¸°**: 5ì´ˆë§ˆë‹¤ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
- **include ì˜µì…˜**: all(ì „ì²´) | k6_only(k6ë§Œ) | resources_only(ë¦¬ì†ŒìŠ¤ë§Œ)""",
           )
async def sse_k6data(
        job_name: str = Path(..., description="í…ŒìŠ¤íŠ¸ ì‹¤ì‹œê°„ ë°ì´í„° ì¶”ì  ìš©ë„ë¡œ ì‚¬ìš©í•  job ì´ë¦„"),
        include: str = Query("all", description="í¬í•¨í•  ë©”íŠ¸ë¦­ íƒ€ì…: all(ê¸°ë³¸)|k6_only|resources_only")
):
    """
    **Server-Sent Events (SSE) ìŠ¤íŠ¸ë¦¬ë°**: k6 ë¶€í•˜í…ŒìŠ¤íŠ¸ì™€ ì„œë²„ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
    
    ## ğŸ”— SSE ì—°ê²° ë°©ë²•
    ```javascript
    const eventSource = new EventSource('/sse/k6data/my-test-job?include=all');
    eventSource.onmessage = function(event) {
        const data = JSON.parse(event.data);
        console.log('ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­:', data);
    };
    ```
    
    ## ğŸ“Š ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
    ```json
    {
        "timestamp": "2025-09-08T12:34:56.789+09:00",
        "test_progress": {
            "duration_seconds": 145,
            "total_duration_seconds": 300,
            "progress_percentage": 48.3
        },
        "overall": {
            "tps": 125.6,
            "vus": 50,
            "response_time": 145.2,
            "error_rate": 0.5
        },
        "scenarios": [
            {
                "name": "get_users",
                "tps": 62.3,
                "vus": 25,
                "response_time": 140.1,
                "error_rate": 0.2
            }
        ],
        "resources": [
            {
                "pod_name": "api-server-123",
                "service_type": "SERVER",
                "usage": {
                    "cpu_percent": 45.2,
                    "memory_percent": 67.8,
                    "cpu_is_predicted": false,
                    "memory_is_predicted": false
                },
                "actual_usage": {
                    "cpu_millicores": 452.5,
                    "memory_mb": 678.3
                },
                "specs": {
                    "cpu_request_millicores": 500,
                    "cpu_limit_millicores": 1000,
                    "memory_request_mb": 512,
                    "memory_limit_mb": 1024
                },
                "prediction_info": {
                    "cpu_streak": 0,
                    "memory_streak": 0,
                    "cpu_confidence": 1.0,
                    "memory_confidence": 1.0
                }
            }
        ]
    }
    ```
    
    ## ğŸ“ í•„ë“œ ì„¤ëª…
    
    ### K6 ë©”íŠ¸ë¦­
    - **tps**: Transactions Per Second (ì´ˆë‹¹ íŠ¸ëœì­ì…˜ ìˆ˜)
    - **vus**: Virtual Users (ê°€ìƒ ì‚¬ìš©ì ìˆ˜)
    - **response_time**: í‰ê·  ì‘ë‹µì‹œê°„ (ms)
    - **error_rate**: ì˜¤ë¥˜ìœ¨ (%)
    
    ### ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­
    - **usage.cpu_percent**: CPU ì‚¬ìš©ë¥  (limit ê¸°ì¤€ %)
    - **usage.memory_percent**: Memory ì‚¬ìš©ë¥  (limit ê¸°ì¤€ %)
    - **actual_usage.cpu_millicores**: ì‹¤ì œ CPU ì‚¬ìš©ëŸ‰ (millicores)
    - **actual_usage.memory_mb**: ì‹¤ì œ Memory ì‚¬ìš©ëŸ‰ (MB)
    - **specs**: Podì˜ ë¦¬ì†ŒìŠ¤ request/limit ì„¤ì •ê°’
    - **prediction_info**: ì˜ˆì¸¡ ëª¨ë¸ ì‹ ë¢°ë„ ì •ë³´
    
    ## âš™ï¸ Parameters
    - **job_name**: k6 í…ŒìŠ¤íŠ¸ Job ì´ë¦„ (ì˜ˆ: "load-test-20250908-123456")
    - **include**: í¬í•¨í•  ë©”íŠ¸ë¦­ íƒ€ì…
        - `"all"` (ê¸°ë³¸): k6 + resource ë©”íŠ¸ë¦­ ëª¨ë‘ í¬í•¨
        - `"k6_only"`: k6 ë©”íŠ¸ë¦­ë§Œ í¬í•¨  
        - `"resources_only"`: resource ë©”íŠ¸ë¦­ë§Œ í¬í•¨ (í–¥í›„ êµ¬í˜„)
        
    ## ğŸ”„ ì—…ë°ì´íŠ¸ ì£¼ê¸°
    - **5ì´ˆë§ˆë‹¤** ìµœì‹  ë©”íŠ¸ë¦­ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
    - ì—°ê²°ì´ ëŠì–´ì§€ë©´ ìë™ìœ¼ë¡œ ì¬ì—°ê²° ì‹œë„
    """
    # íŒŒë¼ë¯¸í„° ê²€ì¦ ë° ë³€í™˜
    include = include.lower()
    valid_includes = {"all", "k6_only", "resources_only"}
    
    if include not in valid_includes:
        logger.warning(f"Invalid include parameter '{include}', using 'all'")
        include = "all"
    
    # include íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ì„¤ì •
    if include == "all":
        include_resources = True
    elif include == "k6_only":
        include_resources = False
    elif include == "resources_only":
        # TODO: í–¥í›„ resources_only êµ¬í˜„ì‹œ ë³„ë„ ë¡œì§ ì¶”ê°€
        logger.info(f"resources_only mode requested for job {job_name} (not yet fully implemented)")
        include_resources = True
    else:
        include_resources = True  # fallback
    
    logger.info(f"Starting SSE for job {job_name} with include={include} (resources={include_resources})")
    
    return StreamingResponse(
        event_stream(job_name, include_resources), 
        media_type="text/event-stream"
    )


# ========== Resource Metrics Functions ==========

# ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­ ë²„í¼ë“¤ (job_name -> pod_name -> metric_type -> SmartMetricsBuffer)
resource_metrics_buffers: Dict[str, Dict[str, Dict[str, SmartMetricsBuffer]]] = {}


def get_pod_cpu_usage_millicores(pod_name: str) -> Optional[float]:
    """
    Podì˜ í˜„ì¬ CPU ì‚¬ìš©ëŸ‰ ì¡°íšŒ (millicores ë‹¨ìœ„)
    
    Args:
        pod_name: Pod ì´ë¦„
        
    Returns:
        float: CPU ì‚¬ìš©ëŸ‰ (millicores ë‹¨ìœ„) ë˜ëŠ” None
    """
    try:
        query = f'''
            SELECT non_negative_derivative(last("container_cpu_usage_seconds_total"), 1s) * 1000 as cpu_millicores
            FROM "cadvisor_metrics"
            WHERE "pod" = '{pod_name}' AND "container" = '' AND "image" = ''
            AND time > now() - 30s
            GROUP BY time(5s) fill(null)
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('cpu_millicores') is not None:
            cpu_millicores = float(points[0]['cpu_millicores'])
            logger.debug(f"Pod {pod_name} CPU usage: {cpu_millicores:.2f} millicores")
            return cpu_millicores
        else:
            logger.debug(f"No CPU data found for pod {pod_name}")
            return None
            
    except Exception as e:
        logger.error(f"Error querying CPU usage for {pod_name}: {e}")
        return None


def get_pod_memory_usage_mb(pod_name: str) -> Optional[float]:
    """
    Podì˜ í˜„ì¬ Memory ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MB ë‹¨ìœ„)
    
    Args:
        pod_name: Pod ì´ë¦„
        
    Returns:
        float: Memory ì‚¬ìš©ëŸ‰ (MB ë‹¨ìœ„) ë˜ëŠ” None
    """
    try:
        query = f'''
            SELECT last("container_memory_usage_bytes") as memory_bytes
            FROM "cadvisor_metrics"
            WHERE "pod" = '{pod_name}' AND "container" = '' AND "image" = ''
            AND time > now() - 30s
        '''
        result = client.query(query)
        points = list(result.get_points())
        
        if points and points[0].get('memory_bytes') is not None:
            memory_bytes = float(points[0]['memory_bytes'])
            memory_mb = memory_bytes / (1024 * 1024)  # bytes to MB
            logger.debug(f"Pod {pod_name} Memory usage: {memory_mb:.2f} MB")
            return memory_mb
        else:
            logger.debug(f"No Memory data found for pod {pod_name}")
            return None
            
    except Exception as e:
        logger.error(f"Error querying memory usage for {pod_name}: {e}")
        return None


def calculate_resource_usage_percentage(actual_usage: Optional[float], limit_value: Optional[float], 
                                      resource_type: str) -> float:
    """
    ì‹¤ì œ ì‚¬ìš©ëŸ‰ì„ limit ê¸°ì¤€ ë°±ë¶„ìœ¨ë¡œ ê³„ì‚°
    
    Args:
        actual_usage: ì‹¤ì œ ì‚¬ìš©ëŸ‰ (millicores ë˜ëŠ” MB)
        limit_value: limit ê°’ (millicores ë˜ëŠ” MB)
        resource_type: 'cpu' ë˜ëŠ” 'memory'
        
    Returns:
        float: ì‚¬ìš©ë¥  ë°±ë¶„ìœ¨ (0-100%)
    """
    if actual_usage is None or limit_value is None or limit_value == 0:
        logger.debug(f"Cannot calculate {resource_type} percentage: "
                    f"usage={actual_usage}, limit={limit_value}")
        return 0.0
    
    usage_percentage = (actual_usage / limit_value) * 100
    result = min(100.0, max(0.0, usage_percentage))  # 0-100% ë²”ìœ„ ì œí•œ
    
    logger.debug(f"{resource_type.upper()} usage: {actual_usage:.2f} / {limit_value:.2f} "
                f"= {result:.2f}%")
    
    return result


def get_pod_resource_usage_percentage(job_name: str, pod_name: str, service_type: str = "SERVER") -> Optional[Dict[str, Any]]:
    """
    Podì˜ CPU/Memory ì‚¬ìš©ë¥ ì„ ë°±ë¶„ìœ¨ë¡œ ì¡°íšŒ (ì˜ˆì¸¡ í¬í•¨)
    
    Args:
        job_name: Job ì´ë¦„
        pod_name: Pod ì´ë¦„
        service_type: ì„œë¹„ìŠ¤ ìœ í˜• (SERVER, DATABASE)
        
    Returns:
        Dict containing usage percentages and metadata
    """
    try:
        # 1. Pod spec ì¡°íšŒ (ìºì‹œ í™œìš©)
        pod_spec_cache = get_pod_spec_cache()
        resource_specs = pod_spec_cache.get_pod_spec(pod_name)
        
        if not resource_specs:
            logger.warning(f"No resource specs found for pod {pod_name}")
            return None
        
        # 2. ì‹¤ì œ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
        actual_cpu = get_pod_cpu_usage_millicores(pod_name)
        actual_memory = get_pod_memory_usage_mb(pod_name)
        
        # 3. ë©”íŠ¸ë¦­ ë²„í¼ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
        if job_name not in resource_metrics_buffers:
            resource_metrics_buffers[job_name] = {}
        
        if pod_name not in resource_metrics_buffers[job_name]:
            resource_metrics_buffers[job_name][pod_name] = {
                'cpu': SmartMetricsBuffer(f"{pod_name}_cpu", "percentage", max_value=100.0),
                'memory': SmartMetricsBuffer(f"{pod_name}_memory", "percentage", max_value=100.0)
            }
        
        buffers = resource_metrics_buffers[job_name][pod_name]
        
        # 4. CPU ì‚¬ìš©ë¥  ê³„ì‚°/ì˜ˆì¸¡
        if actual_cpu is not None:
            cpu_percent = calculate_resource_usage_percentage(
                actual_cpu, resource_specs.get('cpu_limit_millicores'), 'cpu'
            )
            buffers['cpu'].add_value(cpu_percent, predicted=False)
            cpu_is_predicted = False
        else:
            cpu_percent = buffers['cpu'].predict_next()
            if cpu_percent is not None:
                buffers['cpu'].add_value(cpu_percent, predicted=True)
                cpu_is_predicted = True
            else:
                cpu_percent = 0.0
                cpu_is_predicted = False
        
        # 5. Memory ì‚¬ìš©ë¥  ê³„ì‚°/ì˜ˆì¸¡
        if actual_memory is not None:
            memory_percent = calculate_resource_usage_percentage(
                actual_memory, resource_specs.get('memory_limit_mb'), 'memory'
            )
            buffers['memory'].add_value(memory_percent, predicted=False)
            memory_is_predicted = False
        else:
            memory_percent = buffers['memory'].predict_next()
            if memory_percent is not None:
                buffers['memory'].add_value(memory_percent, predicted=True)
                memory_is_predicted = True
            else:
                memory_percent = 0.0
                memory_is_predicted = False
        
        return {
            'pod_name': pod_name,
            'service_type': service_type,
            'usage': {
                'cpu_percent': round(cpu_percent, 2) if cpu_percent is not None else 0.0,
                'memory_percent': round(memory_percent, 2) if memory_percent is not None else 0.0,
                'cpu_is_predicted': cpu_is_predicted,
                'memory_is_predicted': memory_is_predicted
            },
            'actual_usage': {
                'cpu_millicores': round(actual_cpu, 2) if actual_cpu is not None else None,
                'memory_mb': round(actual_memory, 2) if actual_memory is not None else None
            },
            'specs': {
                'cpu_request_millicores': resource_specs.get('cpu_request_millicores'),
                'cpu_limit_millicores': resource_specs.get('cpu_limit_millicores'),
                'memory_request_mb': resource_specs.get('memory_request_mb'),
                'memory_limit_mb': resource_specs.get('memory_limit_mb')
            },
            'prediction_info': {
                'cpu_streak': buffers['cpu'].prediction_streak,
                'memory_streak': buffers['memory'].prediction_streak,
                'cpu_confidence': buffers['cpu'].confidence[-1] if buffers['cpu'].confidence else 1.0,
                'memory_confidence': buffers['memory'].confidence[-1] if buffers['memory'].confidence else 1.0
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting resource usage for pod {pod_name}: {e}")
        return None



def get_job_pods_from_scenarios(job_name: str) -> List[str]:
    """
    Jobì˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê´€ë ¨ Podë“¤ì„ ì¶”ì¶œ (Pod ì´ë¦„ë§Œ ë°˜í™˜)
    
    Args:
        job_name: Job ì´ë¦„
        
    Returns:
        List[str]: Pod ì´ë¦„ ëª©ë¡
    """
    from app.services.infrastructure.server_infra_service import get_job_pods_with_service_types
    
    pod_info_list = get_job_pods_with_service_types(job_name)
    pod_names = [pod_info["pod_name"] for pod_info in pod_info_list]
    
    logger.info(f"Extracted pod names for job {job_name}: {pod_names}")
    return pod_names


def collect_resource_metrics(job_name: str) -> Optional[List[Dict[str, Any]]]:
    """
    Jobì˜ ì „ì²´ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (í”Œë« ë°°ì—´ë¡œ ë°˜í™˜)
    
    Args:
        job_name: Job ì´ë¦„
        
    Returns:
        List[Dict] containing individual pod metrics or None if no data
    """
    try:
        # 1. DBì—ì„œ Pod ëª©ë¡ê³¼ service_type ì¡°íšŒ
        pod_info_list = get_job_pods_with_service_types(job_name)
        
        if not pod_info_list:
            logger.warning(f"No pods found for job {job_name}")
            return None
        
        # 2. ê°œë³„ Pod ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (service_type í¬í•¨)
        pod_metrics = []
        
        for pod_info in pod_info_list:
            pod_name = pod_info["pod_name"]
            service_type = pod_info["service_type"]
            
            pod_metric = get_pod_resource_usage_percentage(job_name, pod_name, service_type)
            if pod_metric:
                pod_metrics.append(pod_metric)
                logger.debug(f"Collected metrics for {pod_name} (type: {service_type})")
            else:
                logger.warning(f"Failed to collect metrics for {pod_name}")
        
        if not pod_metrics:
            logger.warning(f"No valid pod metrics for job {job_name}")
            return None
        
        logger.info(f"Collected metrics for {len(pod_metrics)} pods in job {job_name}")
        return pod_metrics
        
    except Exception as e:
        logger.error(f"Error collecting resource metrics for job {job_name}: {e}")
        return None


def cleanup_job_metrics_buffers(job_name: str) -> int:
    """
    Job ì™„ë£Œì‹œ ê´€ë ¨ ë©”íŠ¸ë¦­ ë²„í¼ë“¤ ì •ë¦¬
    
    Args:
        job_name: ì •ë¦¬í•  Job ì´ë¦„
        
    Returns:
        int: ì •ë¦¬ëœ ë²„í¼ ìˆ˜
    """
    if job_name in resource_metrics_buffers:
        pod_count = len(resource_metrics_buffers[job_name])
        del resource_metrics_buffers[job_name]
        logger.info(f"Cleaned up resource metrics buffers for job {job_name} ({pod_count} pods)")
        return pod_count
    
    return 0

def parse_duration_to_seconds(duration_str: str) -> int:
    """
    k6 duration ë¬¸ìì—´ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜

    Args:
        duration_str: "60s", "120s", "1m", "2h", "30m" ë“±

    Returns:
        int: ì´ˆ ë‹¨ìœ„ ì‹œê°„

    Examples:
        parse_duration_to_seconds("60s") -> 60
        parse_duration_to_seconds("2m") -> 120
        parse_duration_to_seconds("1h") -> 3600
    """
    if not duration_str or not isinstance(duration_str, str):
        return 0

    duration_str = duration_str.strip()
    if not duration_str:
        return 0

    # ìˆ«ìì™€ ë‹¨ìœ„ ë¶„ë¦¬
    import re
    match = re.match(r'^(\d+)([smh]?)$', duration_str.lower())

    if not match:
        logger.warning(f"Invalid duration format: {duration_str}")
        return 0

    value = int(match.group(1))
    unit = match.group(2) or 's'  # ë‹¨ìœ„ê°€ ì—†ìœ¼ë©´ ì´ˆë¡œ ê°„ì£¼

    # ë‹¨ìœ„ë³„ ë³€í™˜
    unit_multipliers = {
        's': 1,      # seconds
        'm': 60,     # minutes
        'h': 3600    # hours
    }

    return value * unit_multipliers.get(unit, 1)


def get_duration_seconds(tested_at: datetime) -> int :
    now = datetime.now()
    diff = now - tested_at
    return int(diff.total_seconds())

def get_total_duration_seconds(test_history: TestHistoryModel) -> int:
    """
    í…ŒìŠ¤íŠ¸ íˆìŠ¤í† ë¦¬ì˜ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ì¤‘ ê°€ì¥ ê¸´ ì´ ì‹¤í–‰ ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ê³„ì‚°

    Args:
        test_history: í…ŒìŠ¤íŠ¸ íˆìŠ¤í† ë¦¬ ëª¨ë¸

    Returns:
        int: ìµœëŒ€ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    """
    scenarios: List[ScenarioHistoryModel] = test_history.scenarios
    max_duration = 0

    for scenario in scenarios:
        stages = scenario.stages
        total_duration = 0

        # ê° ì‹œë‚˜ë¦¬ì˜¤ì˜ ëª¨ë“  ìŠ¤í…Œì´ì§€ duration í•©ê³„ ê³„ì‚°
        for stage in stages:
            stage_duration = parse_duration_to_seconds(stage.duration)
            total_duration += stage_duration

        # ê°€ì¥ ê¸´ ì‹œë‚˜ë¦¬ì˜¤ì˜ durationì„ max_durationìœ¼ë¡œ ì„¤ì •
        if total_duration > max_duration:
            max_duration = total_duration

    logger.debug(f"Calculated max scenario duration: {max_duration}s from {len(scenarios)} scenarios")
    return max_duration
